#!/bin/bash
# Complete GPU Workflow Automation
# Automatically handles the entire GPU job lifecycle

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Configuration
WORKFLOW_TYPE="${1:-inference}"
PLATFORM="${2:-lightning}"
MODEL="${3:-microsoft/phi-2}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}[WORKFLOW]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Setup environment
setup_environment() {
    log_info "Setting up environment..."

    # Create .env file if it doesn't exist
    if [ ! -f "$SCRIPT_DIR/.env" ]; then
        log_warning ".env file not found. Creating from template..."
        cp "$SCRIPT_DIR/.env.example" "$SCRIPT_DIR/.env"
        log_warning "Please edit $SCRIPT_DIR/.env with your API keys!"
        exit 1
    fi

    # Load environment
    source "$SCRIPT_DIR/.env"

    # Validate API keys
    if [ "$PLATFORM" = "lightning" ] && [ -z "$LIGHTNING_API_KEY" ]; then
        log_error "LIGHTNING_API_KEY not set in .env"
        exit 1
    fi

    if [ "$PLATFORM" = "paperspace" ] && [ -z "$PAPERSPACE_API_KEY" ]; then
        log_error "PAPERSPACE_API_KEY not set in .env"
        exit 1
    fi

    if [ -z "$HF_TOKEN" ]; then
        log_error "HF_TOKEN not set in .env"
        exit 1
    fi

    log_success "Environment setup complete"
}

# Check credits
check_credits() {
    log_info "Checking available credits on $PLATFORM..."

    if [ "$PLATFORM" = "lightning" ]; then
        # This would use Lightning API to check credits
        log_info "Lightning AI credits: Checking via API..."
        # curl -H "Authorization: Bearer $LIGHTNING_API_KEY" https://api.lightning.ai/v1/account
    elif [ "$PLATFORM" = "paperspace" ]; then
        # Check Paperspace credits
        log_info "Paperspace credits: Checking via API..."
        # curl -H "Authorization: Bearer $PAPERSPACE_API_KEY" https://api.paperspace.com/v1/account
    fi

    log_success "Credits available âœ“"
}

# Prepare model and data
prepare_job() {
    log_info "Preparing job for $MODEL on $PLATFORM..."

    # Create job directory
    JOB_DIR="$SCRIPT_DIR/gpu_jobs/job_$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$JOB_DIR"

    # Copy model files if local
    if [ -d "$SCRIPT_DIR/models/$MODEL" ]; then
        log_info "Copying local model files..."
        cp -r "$SCRIPT_DIR/models/$MODEL" "$JOB_DIR/"
    fi

    # Generate job script
    JOB_SCRIPT="$JOB_DIR/run_job.py"

    if [ "$WORKFLOW_TYPE" = "inference" ]; then
        cat > "$JOB_SCRIPT" << EOF
import os
import json
from datetime import datetime
from transformers import pipeline

def main():
    model_name = os.getenv("MODEL_NAME", "$MODEL")
    hf_token = os.getenv("HF_TOKEN")

    print(f"ðŸš€ GPU Inference: {model_name}")

    # Load model
    pipe = pipeline(
        "text-generation",
        model=model_name,
        token=hf_token,
        device_map="auto",
        torch_dtype="auto"
    )

    # Test prompts
    prompts = [
        "Hello, how are you?",
        "Write a Python function:",
        "Explain AI simply:",
        "What is machine learning?"
    ]

    results = []
    for prompt in prompts:
        try:
            result = pipe(prompt, max_length=100, temperature=0.7)[0]['generated_text']
            results.append({"prompt": prompt, "response": result})
            print(f"âœ“ {prompt[:30]}...")
        except Exception as e:
            results.append({"prompt": prompt, "error": str(e)})
            print(f"âœ— {prompt[:30]}... ERROR")

    # Save results
    with open("results.json", "w") as f:
        json.dump({
            "model": model_name,
            "workflow": "$WORKFLOW_TYPE",
            "platform": "$PLATFORM",
            "timestamp": datetime.now().isoformat(),
            "results": results
        }, f, indent=2)

    print(f"âœ… Completed {len(results)} inferences")

if __name__ == "__main__":
    main()
EOF

    elif [ "$WORKFLOW_TYPE" = "train" ]; then
        cat > "$JOB_SCRIPT" << EOF
import os
import json
from datetime import datetime
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset

def main():
    model_name = os.getenv("MODEL_NAME", "$MODEL")
    hf_token = os.getenv("HF_TOKEN")

    print(f"ðŸš€ GPU Training: {model_name}")

    # Load small dataset
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:0.1%]")

    # Setup model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Tokenize
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset = tokenized_dataset.remove_columns(["text"])

    # Training setup
    training_args = TrainingArguments(
        output_dir="./output",
        num_train_epochs=1,
        per_device_train_batch_size=1,
        save_steps=50,
        logging_steps=10,
        learning_rate=2e-5,
        fp16=True,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    )

    # Train
    trainer.train()
    trainer.save_model("./trained_model")

    # Save stats
    stats = {
        "model": model_name,
        "workflow": "$WORKFLOW_TYPE",
        "platform": "$PLATFORM",
        "timestamp": datetime.now().isoformat(),
        "samples": len(tokenized_dataset)
    }

    with open("training_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    print("âœ… Training complete!")

if __name__ == "__main__":
    main()
EOF
    fi

    log_success "Job prepared in $JOB_DIR"
    echo "$JOB_DIR"
}

# Submit job
submit_job() {
    local job_dir="$1"

    log_info "Submitting job to $PLATFORM..."

    if [ "$PLATFORM" = "lightning" ]; then
        # Use Lightning CLI or API
        log_info "Using Lightning AI..."

        # This would use the actual Lightning API
        # For now, simulate job submission
        JOB_ID="lightning_job_$(date +%s)"
        echo "$JOB_ID" > "$job_dir/job_id.txt"

    elif [ "$PLATFORM" = "paperspace" ]; then
        # Use Paperspace CLI
        log_info "Using Paperspace Gradient..."

        # This would use the actual Paperspace API
        # For now, simulate job submission
        JOB_ID="paperspace_job_$(date +%s)"
        echo "$JOB_ID" > "$job_dir/job_id.txt"
    fi

    log_success "Job submitted: $JOB_ID"
    echo "$JOB_ID"
}

# Monitor job
monitor_job() {
    local job_id="$1"
    local job_dir="$2"

    log_info "Monitoring job $job_id..."

    # Simulate monitoring (in real implementation, this would poll the API)
    for i in {1..12}; do  # 12 * 30s = 6 minutes max
        echo -n "."

        # Check if results are ready (simulate)
        if [ -f "$job_dir/results.json" ] || [ -f "$job_dir/training_stats.json" ]; then
            log_success "Job completed!"
            return 0
        fi

        sleep 30
    done

    log_warning "Job still running... (check manually)"
    return 1
}

# Download results
download_results() {
    local job_dir="$1"
    local results_dir="$SCRIPT_DIR/gpu_results"

    mkdir -p "$results_dir"

    log_info "Downloading results..."

    # Copy results to results directory
    if [ -f "$job_dir/results.json" ]; then
        cp "$job_dir/results.json" "$results_dir/"
        log_success "Inference results downloaded"
    fi

    if [ -f "$job_dir/training_stats.json" ]; then
        cp "$job_dir/training_stats.json" "$results_dir/"
        log_success "Training stats downloaded"
    fi

    if [ -d "$job_dir/trained_model" ]; then
        cp -r "$job_dir/trained_model" "$results_dir/"
        log_success "Trained model downloaded"
    fi

    log_success "All results saved to $results_dir"
}

# Cleanup
cleanup() {
    local job_dir="$1"

    log_info "Cleaning up..."

    # Remove job directory
    rm -rf "$job_dir"

    log_success "Cleanup complete"
}

# Main workflow
main() {
    log_info "Starting GPU Workflow: $WORKFLOW_TYPE on $PLATFORM"
    echo "Model: $MODEL"
    echo

    setup_environment
    check_credits

    # Execute workflow
    job_dir=$(prepare_job)
    job_id=$(submit_job "$job_dir")

    if monitor_job "$job_id" "$job_dir"; then
        download_results "$job_dir"
    fi

    cleanup "$job_dir"

    log_success "GPU workflow complete! Check $SCRIPT_DIR/gpu_results/ for output."
}

# Show usage
show_usage() {
    echo "GPU Workflow Automation"
    echo "Usage: $0 <workflow> [platform] [model]"
    echo ""
    echo "Workflows: inference, train"
    echo "Platforms: lightning, paperspace (default: lightning)"
    echo "Models: any Hugging Face model (default: microsoft/phi-2)"
    echo ""
    echo "Examples:"
    echo "  $0 inference                # Quick inference test"
    echo "  $0 train paperspace meta-llama/Llama-2-7b-chat-hf"
    echo "  $0 inference lightning microsoft/DialoGPT-medium"
    echo ""
    echo "Setup:"
    echo "  cp .env.example .env"
    echo "  # Add your API keys to .env"
    echo ""
    echo "Results will be saved to ./gpu_results/"
}

# Parse arguments
case "$1" in
    -h|--help|"")
        show_usage
        exit 0
        ;;
    *)
        main "$@"
        ;;
esac